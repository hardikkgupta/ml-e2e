# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lByrAGmCbAGSuXZIksITHbc4T7Z-nNxe
"""

import os
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
import joblib

dir = '/content/drive/MyDrive/data/'


def train_and_save_model():
    train_path = os.path.join(dir, 'train.csv')
    test_path = os.path.join(dir, 'test.csv')

    train_df = pd.read_csv(train_path)
    test_df = pd.read_csv(test_path)

    print("Training Data Shape:", train_df.shape)
    print("Test Data Shape:", test_df.shape)

    drop_columns = ['PassengerId', 'Name', 'Ticket', 'Cabin']
    # Dropping columns and 'Survived'
    X = train_df.drop(columns=drop_columns + ['Survived'])
    y = train_df['Survived']

    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()
    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()
    print("\nNumerical Columns:", numerical_cols)
    print("Categorical Columns:", categorical_cols)

    numerical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])
    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='most_frequent')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])
    preprocessor = ColumnTransformer(transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

    pipeline = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', LogisticRegression(max_iter=1000))
    ])

    pipeline.fit(X, y)
    os.makedirs('app', exist_ok=True)
    model_path = os.path.join('app', 'model.pkl')
    joblib.dump(pipeline, model_path)
    print(f'Model saved to {model_path}')

    if 'Survived' in test_df.columns:
        evaluate_test_set(pipeline, test_df, drop_columns)
    else:
        print("\nTest labels (Survived column) not found. Preprocessing needed")


def evaluate_test_set(pipeline, test_df, drop_columns):
    X_test = test_df.drop(columns=drop_columns + ['Survived'])
    y_test = test_df['Survived']

    y_pred = pipeline.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f'\nTest Set Accuracy: {accuracy:.3f}')


if __name__ == "__main__":
    train_and_save_model()
